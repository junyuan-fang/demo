{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8040a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward (incoming_gradients):\n",
    "    self.Tensor.grad = incoming_gradients\n",
    "\n",
    "    for inp in self.inputs:\n",
    "        if inp.grad_fn is not None:\n",
    "            new_incoming_gradients = incoming_gradient * local_grad(self.Tensor, inp)\n",
    "            \n",
    "            inp.grad_fn.backward(new_incoming_gradients)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98418f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a \n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c \n",
    "\n",
    "L = (10 - d).sum()# if no 'sum()' -> \"grad can be implicitly created only for scalar outputs\"\n",
    "\n",
    "# Replace L.backward() with \n",
    "L.backward(torch.ones(L.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79d78ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2475,  1.2745, -0.2717],\n",
       "        [ 0.0703,  2.3788, -0.1542],\n",
       "        [ 1.5116,  0.0894,  0.8308]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8f8e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.5000, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([1.0,2.0],requires_grad=True)\n",
    "y = (x + 2)**2\n",
    "z = torch.mean(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071e26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()#因为z 这之后是 scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241ee07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 36.,  64., 100.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0],requires_grad=True)\n",
    "y = (x + 2)**2\n",
    "z = 4*y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054ff18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(torch.tensor([1.,1.,1.])) # 因为z 这之后不是是 scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d34e1d",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "首先，在创建优化器对象的时候，要传入网络模型的参数，并设置学习率等优化方法的参数。然后使用函数zero_grad将梯度置为零。接着调用函数backward来进行反向传播计算梯度。最后使用优化器的step函数来更新参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29d708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "# A Toy Dataset\n",
    "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]])\n",
    "target = torch.tensor([[0],[0],[1],[1.]])\n",
    "\n",
    "# A Toy Model\n",
    "model = nn.Linear(2,1)\n",
    "\n",
    "def train():\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1) #Implements stochastic gradient descent\n",
    "    for iter in range(20):\n",
    "\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        pred = model(data)\n",
    "\n",
    "        # 3) calculate how much we missed\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) change those weights\n",
    "        opt.step()\n",
    "\n",
    "        # 6) print our progress\n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf679cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0176)\n",
      "tensor(1.5890)\n",
      "tensor(1.0315)\n",
      "tensor(0.6920)\n",
      "tensor(0.4689)\n",
      "tensor(0.3200)\n",
      "tensor(0.2201)\n",
      "tensor(0.1525)\n",
      "tensor(0.1066)\n",
      "tensor(0.0750)\n",
      "tensor(0.0532)\n",
      "tensor(0.0381)\n",
      "tensor(0.0274)\n",
      "tensor(0.0199)\n",
      "tensor(0.0145)\n",
      "tensor(0.0107)\n",
      "tensor(0.0079)\n",
      "tensor(0.0058)\n",
      "tensor(0.0043)\n",
      "tensor(0.0032)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
